{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bded025-7b81-4c1b-b05e-ebe2b0eb9146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883a6d10-f6e3-4dfa-904e-f109de0f0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\anjup\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c011043-3cde-43c6-a2d8-6d4a3687304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f8ec6b-86f5-423c-b074-50d7579b6107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"C:\\\\Users\\\\anjup\\\\Downloads\\\\abcnews-date-text.csv\")\n",
    "news_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a926ea-2dec-42a0-a0cd-4c527b597c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 20000 # The number of sample to use\n",
    "\n",
    "df = news_data.sample(NUM_SAMPLES, replace=False, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b150918-f4cd-47c5-afd6-3fe7e84f37bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20181017</td>\n",
       "      <td>virtual reality trial ahead of fire season in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20070131</td>\n",
       "      <td>farmers prepare for ec funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20140810</td>\n",
       "      <td>the sunday inquisition august 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20130221</td>\n",
       "      <td>news csg reax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20140806</td>\n",
       "      <td>rosetta spacecraft on final approach to comet ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20181017  virtual reality trial ahead of fire season in ...\n",
       "1      20070131                     farmers prepare for ec funding\n",
       "2      20140810                   the sunday inquisition august 10\n",
       "3      20130221                                      news csg reax\n",
       "4      20140806  rosetta spacecraft on final approach to comet ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020a7c77-414f-477b-888f-71b19401eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained(\"google/muril-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7da0e7-f5fa-4de3-91cc-eb0620ba928b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhdbscan\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- Step 1: Preprocess text ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    # Load and preprocess\n",
    "docs_raw = df['headline_text'].dropna().tolist()\n",
    "texts = [preprocess(doc) for doc in docs_raw]\n",
    "docs = [\" \".join(t) for t in texts]\n",
    "\n",
    "# --- Step 2: Generate BERT or MuRIL Embeddings ---\n",
    "# You can swap the model to 'ai4bharat/indic-sentence-bert' for MuRIL\n",
    "model = model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "embeddings = model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# --- Step 3: HDBSCAN Clustering ---\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# --- Step 4: TF-IDF for topic word extraction ---\n",
    "valid_idx = labels != -1\n",
    "filtered_docs = np.array(docs)[valid_idx]\n",
    "filtered_labels = labels[valid_idx]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(filtered_docs)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "cluster_docs = defaultdict(list)\n",
    "for doc, label in zip(filtered_docs, filtered_labels):\n",
    "    cluster_docs[label].append(doc)\n",
    "    topic_words = []\n",
    "for label, cluster_texts in cluster_docs.items():\n",
    "    tfidf_cluster = vectorizer.fit_transform(cluster_texts)\n",
    "    mean_tfidf = np.asarray(tfidf_cluster.mean(axis=0)).flatten()\n",
    "    top_n_idx = mean_tfidf.argsort()[-10:][::-1]\n",
    "    topic = [terms[i] for i in top_n_idx]\n",
    "    topic_words.append(topic)\n",
    "\n",
    "# --- Step 5: Coherence Score ---\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topic_words,\n",
    "    texts=texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v',\n",
    "    topn=10\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# --- Step 6: Silhouette Score ---\n",
    "if np.unique(filtered_labels).size > 1:\n",
    "    sil_score = silhouette_score(embeddings[valid_idx], filtered_labels)\n",
    "else:\n",
    "    sil_score = None\n",
    "\n",
    "# --- Step 7: Output Results ---\n",
    "print(\"Coherence Score (MuRIL Hybrid):\", round(coherence_score, 4))\n",
    "print(\"Silhouette Score (MuRIL Hybrid):\", round(sil_score, 4) if sil_score else \"Not enough distinct clusters\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a209b1b-517c-494b-a4d6-11ab5e610a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = sorted(set(labels[labels != -1]))  # exclude noise (-1)\n",
    "\n",
    "user_type_map = {cluster: f\"UserType_{i}\" for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# Map cluster labels to user types\n",
    "user_type_predictions = [user_type_map[label] if label in user_type_map else \"Unknown\" for label in labels]\n",
    "\n",
    "# Add to DataFrame\n",
    "df = df.loc[df['headline_text'].notna()].copy()  # Ensure alignment\n",
    "df['predicted_user_type'] = user_type_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af13be-6d7c-4007-b4c2-44d112359726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['topic_label'] = label_encoder.fit_transform(df['predicted_user_type'])\n",
    "\n",
    "# Final labels\n",
    "y = df['topic_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab7630-995b-44e4-9161-f3942ae5a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only non-noise docs\n",
    "X = embeddings[labels != -1]\n",
    "y = y[labels != -1]  # ensure same indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863999e8-28f6-4d4d-83c2-6adf47e8ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "X_reduced = pca.fit_transform(X)  # Before train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaee249-55e3-45ec-8920-a4b8e193bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe40b0-e484-4ce6-b2b0-c9b46dcccf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c2819-31af-47c7-a088-6d8c41e1f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=0.1, penalty='l2', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3658c22-5f48-4d20-9144-28d32a79fb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy (Logistic): 0.9979591836734695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(log_reg, X_reduced, y, cv=5)\n",
    "print(\"Cross-validated accuracy (Logistic):\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60655591-b8c6-4ee9-8a21-eadc20774c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
